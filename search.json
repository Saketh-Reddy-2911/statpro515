[
  {
    "objectID": "varun.html#education",
    "href": "varun.html#education",
    "title": "3.3 Varun Tej Mallekedi",
    "section": "Education",
    "text": "Education\nGeorge Mason University, Virginia | Master’s in Data Analytics Engineering | Jan 2024 - Dec 2025\nMVSR College of Engineering, Hyderabad | Bachelor’s in Civil Engineering | Aug 2019 - May 2023",
    "crumbs": [
      "3. Group Members",
      "3.3 Varun Tej Mallekedi"
    ]
  },
  {
    "objectID": "saketh.html#education",
    "href": "saketh.html#education",
    "title": "3.2 Saketh Reddy Puchakayala",
    "section": "Education",
    "text": "Education\nGeorge Mason University, Virginia | Master’s in Data Analytics Engineering | Jan 2024 - Dec 2025\nVasavi College of Engineering, Hyderabad | Bachelor’s in Mechanical Engineering | Aug 2019 - May 2023",
    "crumbs": [
      "3. Group Members",
      "3.2 Saketh Reddy Puchakayala"
    ]
  },
  {
    "objectID": "question3.html",
    "href": "question3.html",
    "title": "2.3 Research Question: 3",
    "section": "",
    "text": "3. Can a song recommendation system be developed to suggest personalized song recommendations to users based on their preferences and songs?\n\n\nAim :\nThe aim is to implement a content-based recommendation system for suggesting songs to users based on their preferences and the features of the songs.\n\n\nHypothesis :\nTo generate personalized song recommendations which can be generated for users based on their preferences and song features. The assumption is that songs with similar audio features to the user’s preferences are likely to be preferred by the user. By computing the cosine similarity between the user preferences and the features of songs, the code aims to recommend songs that are most similar to the user’s preferred songs.\n\n\nResearch :\nThis will involve data preprocessing, model training, evaluation, and comparison of collaborative filtering and content-based recommendation approaches. Additionally, the study may investigate hybrid recommendation systems that combine collaborative filtering and content-based techniques to provide more accurate and diverse recommendations.\nThe findings of this study could have implications for the development of personalized recommendation systems in various domains, including music streaming platforms, online retail, and content curation services. By understanding the strengths and limitations of different recommendation techniques, this research aims to contribute to the advancement of personalized recommendation systems tailored to individual user preferences.\n\n\nApproach :\nFirstly, by loading necessary libraries such as dplyr, Matrix, and shiny.\nThe songs dataset is created and it contains information about various songs, including their popularity, title, artist, genre, year, and audio features like BPM, energy, danceability, etc.\nThe cosine_similarity function computes the cosine similarity between two vectors. This function is used to compare the user’s preferences with each song in the data set.\nThe Songs function takes user preferences, the song data set, and an optional parameter top_n (default value is 10) as input. It computes the cosine similarity between the user’s preferences and each song in the data set, orders the songs based on similarity (in descending order), and returns the top top_n songs.\nThe ui variable defines the user interface (UI) of the Shiny application using fluidPage and sidebarLayout. It consists of several slider inputs for the user to specify their preferences and a button to trigger song recommendations. The main panel contains a table (dataTableOutput) to display the recommended songs.\nThe server function defines the server logic of the Shiny application. It contains an observer (observeEvent) that listens for clicks on the “Recommend” button (input$recommendBtn). When the button is clicked, it retrieves the user’s preferences from the slider inputs, computes song recommendations using the Songs function, and renders the recommended songs in the main panel. Running the Shiny application: Finally, the shinyApp function is called with the UI and server logic as arguments to create and run the Shiny application.\n\n\nApplication :\nhttps://projecttt.shinyapps.io/FinalProject/\n\nCode\n\n\n\nlibrary(dplyr)\nlibrary(Matrix)\nlibrary(shiny)\nsongs &lt;- data.frame(\n  Popularity = c(83, 81, 80, 88, 86, 84, 84, 87, 86, 99),\n  Title = c(\"Hey, Soul Sister\", \"A Thousand Years\", \"Titanium (feat. Sia)\", \"Underneath the Tree\", \"All of Me\", \"Thinking out Loud\", \"The Hills\", \"Shape of You\", \"One Kiss (with Dua Lipa)\", \"Memories\"),\n  Artist = c(\"Train\", \"Christina Perri\", \"David Guetta\", \"Kelly Clarkson\", \"John Legend\", \"Ed Sheeran\", \"The Weeknd\", \"Ed Sheeran\", \"Calvin Harris\", \"Maroon 5\"),\n  Genre = c(\"neo mellow\", \"dance pop\", \"dance pop\", \"dance pop\", \"neo mellow\", \"pop\", \"canadian contemporary r&b\", \"pop\", \"dance pop\", \"pop\"),\n  Year = c(\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\"),\n  BPM = c(97, 139, 126, 160, 120, 79, 113, 96, 124, 91),\n  Energy = c(89, 41, 79, 81, 26, 45, 56, 65, 86, 32),\n  Danceability = c(67, 42, 60, 51, 42, 78, 58, 83, 79, 76),\n  Loudness = c(-4, -7, -4, -5, -7, -6, -7, -3, -3, -7),\n  Liveness = c(8, 11, 13, 21, 13, 18, 14, 9, 8, 8),\n  Valence = c(80, 16, 30, 69, 33, 59, 14, 93, 59, 57),\n  Length = c(217, 285, 245, 230, 270, 282, 242, 234, 215, 189),\n  Acousticness = c(19, 31, 7, 0, 92, 47, 7, 58, 4, 84),\n  Speechiness = c(4, 3, 10, 5, 3, 3, 5, 8, 11, 5)\n)\n\ncosine_similarity &lt;- function(x, y) {\n  dot_product &lt;- sum(x * y)\n  magnitude_x &lt;- sqrt(sum(x^2))\n  magnitude_y &lt;- sqrt(sum(y^2))\n  similarity &lt;- dot_product / (magnitude_x * magnitude_y)\n  return(similarity)\n}\n\n\nSongs &lt;- function(Intrest, songs, top_n = 10) {\n  similarities &lt;- apply(songs[, 6:ncol(songs)], 1, function(x) cosine_similarity(Intrest, x))\n  Songs_Interest &lt;- songs[order(similarities, decreasing = TRUE), ][1:top_n, ]\n  return(Songs_Interest)\n}\n\n\nui &lt;- fluidPage(\n  titlePanel(\"Recommendations\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"bpm\", \"BPM:\", min = 50, max = 200, value = 130),\n      sliderInput(\"energy\", \"Energy:\", min = 0, max = 100, value = 85),\n      sliderInput(\"danceability\", \"Danceability:\", min = 0, max = 100, value = 70),\n      sliderInput(\"loudness\", \"Loudness:\", min = -20, max = 0, value = -5),\n      sliderInput(\"liveness\", \"Liveness:\", min = 0, max = 100, value = 10),\n      sliderInput(\"valence\", \"Valence:\", min = 0, max = 100, value = 75),\n      sliderInput(\"length\", \"Length:\", min = 100, max = 400, value = 230),\n      sliderInput(\"acousticness\", \"Acousticness:\", min = 0, max = 100, value = 15),\n      sliderInput(\"speechiness\", \"Speechiness:\", min = 0, max = 100, value = 5),\n      actionButton(\"recommendBtn\", \"Recommend\")\n    ),\n    mainPanel(\n      dataTableOutput(\"recommendations\")\n    )\n  )\n)\n\n\nserver &lt;- function(input, output) {\n  observeEvent(input$recommendBtn, {\n    Intrest &lt;- c(input$bpm, input$energy, input$danceability, input$loudness, input$liveness, input$valence, input$length, input$acousticness, input$speechiness)\n    Songs_Interest &lt;- Songs(Intrest, songs)\n    output$recommendations &lt;- renderDataTable({\n      Songs_Interest[, c(\"Title\", \"Artist\", \"Genre\")]\n    })\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n  \n\n\n\n\n\nConclusion :\nOverall, this code demonstrates the implementation of a content-based recommendation system for suggesting songs to users based on their preferences and the features of the songs. By computing the cosine similarity between the user preferences and each song’s features, the code generates personalized song recommendations tailored to the user’s preferences. This approach aims to enhance the user experience by providing relevant and personalized song suggestions.",
    "crumbs": [
      "2. Research",
      "2.3 Research Question: 3"
    ]
  },
  {
    "objectID": "question1.html",
    "href": "question1.html",
    "title": "2.1 Research Question: 1",
    "section": "",
    "text": "1. Does the fame of artist affect the popularity of the song?\n\n\nAim :\nThis research is to analyze the popularity of artists on the Spotify platform and understand how different subsets of artists, categorized based on their popularity ratings, perform in terms of predictive accuracy.\n\n\nHypothesis :\nThe artists categorized as “Top” in terms of popularity will have the lowest root mean squared error (RMSE) values when using Lasso regression for popularity prediction. Conversely, artists categorized as “Below Average” are expected to have the highest RMSE values.\n\n\nResearch :\nOur research focuses on understanding how the predictive accuracy of popularity models varies across different subsets of artists. By categorizing artists based on their popularity ratings, we aim to uncover patterns in model performance and identify which subsets are better represented by our Lasso regression models.\n\n\nApproach :\nIn our initial step, we imported a data set containing details about artists and their tracks from Spotify. We divided these artists into three groups based on their popularity : “Top,” “Average,” and “Below Average.”\nNext, we employed Lasso regression, a form of linear regression with a penalty term, to forecast the popularity of tracks across different artists. For each subgroup, we partitioned the dataset into training and testing sets and trained a Lasso regression model using the training data.\nTo assess the effectiveness of each model, we calculated the root mean squared error (RMSE) on the testing data. Lower RMSE values indicate higher predictive accuracy.\nFinally, we depicted the RMSE values for each subgroup using a bar chart, enabling a visual comparison of their performance.\n\n\n\nCode\n\n\n&lt;p&gt;data &lt;- read.csv(\"D:/Data Analytics/Sem 1/STAT 515/Final Project/updated_dataset.csv\")\nstr(data)\ntopartists &lt;- c(\"Taylor Swift\", \"Adele\", \"Bruno Mars\", \"Ed Sheeran\",\n                 \"Rihanna\", \"Justin Bieber\", \"Katy Perry\", \"Lady Gaga\",\n                 \"Beyoncé\", \"Eminem\", \"Calvin Harris\", \"Selena Gomez\",\n                 \"Nicki Minaj\", \"Dua Lipa\", \"The Weeknd\", \"One Direction\",\n                 \"Shawn Mendes\", \"Maroon 5\", \"Sia\", \"Lizzo\")\n\naverageartists &lt;- c(\"Alicia Keys\", \"Demi Lovato\", \"Camila Cabello\",\n                     \"Sam Smith\", \"Jonas Brothers\", \"Ellie Goulding\",\n                     \"Halsey\", \"Jason Derulo\", \"Jessie J\", \"Avicii\",\n                     \"Hailee Steinfeld\", \"Zedd\", \"Bastille\", \"Charlie Puth\",\n                     \"Lana Del Rey\", \"Macklemore & Ryan Lewis\", \"John Legend\",\n                     \"Kelly Clarkson\", \"Fifth Harmony\", \"Lana Del Rey\")\n\nbelowaverage_artists &lt;- c(\"Birdy\", \"Sleeping At Last\", \"Cashmere Cat\",\n                           \"Lea Michele\", \"Rudimental\", \"Lilly Wood and The Prick\",\n                           \"A Great Big World\", \"Snakehips\", \"Lost Frequencies\",\n                           \"Joey Montana\", \"Florence + The Machine\", \"Paloma Faith\",\n                           \"Robin Schulz\", \"Years & Years\", \"Tove Lo\", \"Niall Horan\",\n                           \"Alessia Cara\", \"Jonas Blue\", \"Mabel\", \"Silk City\")\ndata$Rating &lt;- ifelse(data$Artist %in% topartists, \"Top\",\n                         ifelse(data$Artist %in% averageartists, \"Average\", \"Below Average\"))\n\nhead(data)\n\n########\n\nlibrary(glmnet)\nlibrary(Matrix)\nlibrary(caret)\nlibrary(ggplot2)\n\nsubsets &lt;- list(\n  Top = top,\n  Average = average,\n  Below_Average = below\n)\n\nrmse_values &lt;- numeric(length(subsets))\n\ncompute_rmse &lt;- function(data) {\n\n  set.seed(123)\n\n  train_indices &lt;- createDataPartition(y = data$Popularity, p = 0.7, list = FALSE)\n  X_train &lt;- data[train_indices, ]\n  X_test &lt;- data[-train_indices, ]\n  y_train &lt;- X_train$Popularity\n  y_test &lt;- X_test$Popularity\n\n  lasso_model &lt;- glmnet(x = as.matrix(X_train[, -c(1, 21)]), y = y_train, alpha = 1)\n\n  predictions &lt;- predict(lasso_model, newx = as.matrix(X_test[, -c(1, 21)]))\n\n  residuals &lt;- y_test - predictions\n\n  sqrt(mean(residuals^2))\n}\n\n\nfor (i in seq_along(subsets)) {\n  rmse_values[i] &lt;- compute_rmse(subsets[[i]])\n  cat(\"RMSE for\", names(subsets)[i], \"subset:\", rmse_values[i], \"\\n\")\n}\n\n\nrmse_data &lt;- data.frame(Subset = names(subsets), RMSE = rmse_values)\n\n\nbarplot &lt;- ggplot(rmse_data, aes(x = Subset, y = RMSE, fill = Subset)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  labs(title = \"Root Mean Squared Error (RMSE) for Different level of artists\",\n       x = \"Type of artists\",\n       y = \"RMSE\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  \nprint(barplot)\n)\n\n\n\n\n\n\nConclusion :\nOur analysis revealed insights into the predictive accuracy of popularity models for different subsets of artists. We found that Lasso regression performed differently across subsets, with “Top” artists generally exhibiting lower RMSE values compared to “Average” and “Below Average” artists. This suggests that popularity prediction vary depending on the level of artist popularity, highlighting the importance of considering artist subsets in predictive modeling on platforms like Spotify.",
    "crumbs": [
      "2. Research",
      "2.1 Research Question: 1"
    ]
  },
  {
    "objectID": "bhuvan.html#education",
    "href": "bhuvan.html#education",
    "title": "3.1 Bhuvan Venkat",
    "section": "Education",
    "text": "Education\nGeorge Mason University, Virginia | Master’s in Data Analytics Engineering | Jan 2024 - Dec 2025\nSreenidhi Institute of Science and Technology, Hyderabad | Bachelor’s in Electronics and Computer Engineering | Aug 2019 - May 2023",
    "crumbs": [
      "3. Group Members",
      "3.1 Bhuvan Venkat"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "1. Spotify Dataset Analysis Project",
    "section": "",
    "text": "1.1 Introduction\nThe Spotify dataset includes various attributes of songs that provide insights into different aspects of each track. The dataset consists of a song’s title and artist, which help identify the track and explore the artist’s popularity and musical style. The genre of the song allows for the examination of musical trends across different styles. The year column indicates when the song was released, providing data for analyzing shifts in musical trends over time. The BPM (Beats Per Minute) measures the song’s tempo and can be analyzed about other features like energy and danceability. Energy, on a scale from 0 to 100, measures the intensity and activity of the song, often correlating with loudness and tempo. Danceability, also on a scale from 0 to 100, represents the suitability of the song for dancing. Loudness, measured in decibels, indicates the overall volume of the track, while liveness estimates the presence of a live audience in the recording, suggesting whether the track is a live performance. Valence measures the positivity or happiness of the song, with higher valence indicating more upbeat tracks. The length of the song, measured in seconds, allows for analysis of trends in song durations across genres and years. Acousticness, on a scale from 0 to 100, measures the presence of acoustic elements in the track, while speechiness gauges the amount of spoken words in the song, suggesting genres like rap. Finally, popularity, on a scale from 0 to 100, indicates the song’s popularity on Spotify, offering insights into what makes a song more appealing based on other features such as genre, energy, and danceability. Collectively, these attributes provide a rich dataset for analyzing popular music, understanding musical trends, and exploring relationships between different song features.\n\nTitle: The name of the track\nArtist: Creator of the track\nGenre: Category of the track\nYear: The year in which the track was released\nBeats per minute (BPM): The measure of speed of the track\nEnergy: The measure of intensity of the track\nDanceability: Indicates how suitable is the track for dancing\nLoudness: The average loudness of the track in decibels (dB). Higher values indicate louder songs.\nLiveness: Measures the presence of an audience in the recording. Higher values suggest a live performance.\nValence: A measure of the positivity of the track’s mood. A higher value means a more positive mood.\nLength: The duration of the track in seconds.\nAcousticness: Indicates the extent to which the track is acoustic. Higher values suggest the presence of acoustic elements.\nSpeechiness: The presence of spoken words in the track. Higher values indicate more spoken content.\nPopularity: A measure of the track’s popularity. Higher values suggest the track is more popu",
    "crumbs": [
      "Introduction",
      "1. Spotify Dataset Analysis Project"
    ]
  },
  {
    "objectID": "question2.html",
    "href": "question2.html",
    "title": "2.2 Research Question: 2",
    "section": "",
    "text": "2. How does acoustics relate to energy levels, Loudness to valence, Energy to BPM and Dance-ability to Energy in songs?\n\n\nAim :\nThe aim of this analysis is to explore the relationships between various musical attributes such as acousticness, energy, loudness, valence, BPM (beats per minute), and danceability using data from a Spotify dataset. By examining these relationships, we aim to understand how these attributes interact with each other, potentially revealing insights into musical preferences and characteristics.\n\n\nHypothesis :\nWe hypothesize that there exist correlations between certain pairs of musical attributes. For instance, we expect that energetic songs may have higher BPM, and we anticipate a potential relationship between acousticness and energy, as acoustic instruments tend to be associated with mellower, less energetic music.\n\n\nApproach :\nWe start by getting data on different musical features from Spotify. Then, we create a tool called calculate_and_plot. This tool helps us see how different features relate to each other by calculating numbers that show how closely linked they are. We use these numbers to make scatter plots, which are like graphs showing the connections between features visually. After that, we look at these graphs and numbers to understand how each musical feature is connected to others. This helps us learn more about how songs are put together and what makes them sound the way they do.\n\n\nResearch :\nAcousticness vs. Energy: We aim to investigate whether there is a correlation between the acousticness and energy of songs. The hypothesis suggests that songs with higher energy levels may have lower acousticness values. Loudness vs. Valence: We seek to explore the relationship between loudness and valence, hypothesizing that louder songs might be associated with more positive moods. Energy vs. BPM: We investigate whether there is a correlation between the energy of a song and its BPM, expecting that higher energy songs may have faster tempos. Danceability vs. Energy: We aim to understand the relationship between danceability and energy, hypothesizing that more energetic songs are likely to be more danceable.\n\n\nConclusion :\n\nThe analysis reveals a negative correlation between acousticness and energy, supporting the hypothesis that less acoustic music tends to be more energetic.\n\nThe examination shows no strong correlation between loudness and valence, suggesting that volume does not strongly influence the mood of a song.\n\nThe analysis indicates a moderate positive correlation between energy and BPM, supporting the hypothesis that more energetic songs tend to have faster tempos.\n\nThe investigation demonstrates a positive correlation between danceability and energy, indicating that more energetic songs are more likely to be danceable.\n\nCode\n\n\n\nlibrary(ggplot2)\nspotify &lt;- read.csv(\"D:/Data Analytics/Sem 1/STAT 515/Final Project/updated_dataset.csv\")\ncalculate_and_plot(spotify$Acousticness, spotify$Energy, \"Acousticness\", \"Energy\")\ncalculate_and_plot(spotify$Loudness, spotify$Valence, \"Loudness\", \"Valence\")\ncalculate_and_plot(spotify$Energy, spotify$BPM, \"Energy\", \"BPM\")\ncalculate_and_plot(spotify$Danceability, spotify$Energy, \"Danceability\", \"Energy\")\n\ncalculate_and_plot &lt;- function(x, y, x_label, y_label) {\n  correlation &lt;- cor(x, y)\n  print(paste(\"Correlation coefficient between\", x_label, \"and\", y_label, \":\", correlation))\n  \n  ggplot(data = spotify, aes(x = x, y = y)) +\n    geom_point(alpha = 0.5) +\n    labs(title = paste(\"Scatter plot of\", x_label, \"vs.\", y_label),\n         x = x_label, y = y_label)\n}",
    "crumbs": [
      "2. Research",
      "2.2 Research Question: 2"
    ]
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "1.2 Research Questions",
    "section": "",
    "text": "1. Does the fame of the artist affect the popularity of the song?\n\n\n2.How does acoustics relate to energy levels, Loudness to valence, Energy to BPM and Dance-ability to Energy in songs?\n\n\n3. Can a song recommendation system be developed to suggest personalized song recommendations to users based on their preferences and songs?",
    "crumbs": [
      "Introduction",
      "1.2 Research Questions"
    ]
  }
]